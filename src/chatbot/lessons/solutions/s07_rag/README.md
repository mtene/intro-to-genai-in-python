# Solution 7: Retrieval Augmented Generation

In the solution, we populate the vector store provided to the [chatbot](chatbot.py) with chunked documents, together with the corresponding embeddings.

Run the [tests](tests.py) in the console and verify that they all pass. Remember that LLMs are non-deterministic, so increase repetition count before drawing conclusions. Also consider setting the `seed` parameter or changing `temperature` and `top_p`.

## Implementation: Document ingestion

First, we load the target text from disk

```python
doc_path = Path(__file__).parents[5] / "data" / "the_great_gatsby.txt"
doc_content = ""
with open(doc_path, encoding="utf-8") as f:
    doc_content = f.read()
```

then split it into non-overlapping paragraphs with length up to 2000 characters

```python
splitter = RecursiveCharacterTextSplitter(
    separators=["\n"],
    chunk_size=2000,
    chunk_overlap=0,
)
doc_chunks = splitter.split_text(doc_content)
```

and, finally, ingest them into the vector store after attaching relevant metadata

```python
self._vectordb.add_documents(
    [
        Document(
            page_content=chunk,
            metadata={"document": doc_path, "paragraph": i + 1},
        )
        for i, chunk in enumerate(doc_chunks)
    ]
)
```

Note that each chunk is associated with the corresponding embeddings, as generated by the embedding model - this is taken care of automatically by LangChain's vector store implementation.

## Verification

Ask questions about the document and observe which chunks are being retrieved and how the model interprets the content.

üè† [Overview](/README.md) | ‚óÄÔ∏è [Back to exercise](/src/chatbot/lessons/exercises/e07_rag/README.md) | ‚ñ∂Ô∏è [Next exercise](/src/chatbot/lessons/exercises/e08_custom_agent/README.md)
---|---|---
