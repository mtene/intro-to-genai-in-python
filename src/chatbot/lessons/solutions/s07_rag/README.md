# Solution 7: Retrieval Augmented Generation

In the solution, we populate the vector store provided to the [chatbot](chatbot.py) with chunked documents, together with the corresponding embeddings.

Run the [tests](tests.py) in the console and verify that they all pass. Remember that LLMs are non-deterministic, so increase repetition count before drawing conclusions. Also consider setting the `seed` parameter or changing `temperature` and `top_p`.

## Implementation: Document ingestion

First, we load the target text from disk

```python
doc_path = Path(__file__).parents[5] / "data" / "the_great_gatsby.txt"
doc_content = ""
with open(doc_path, encoding="utf-8") as f:
    doc_content = f.read()
```

then split it into non-overlapping paragraphs with length up to 2000 characters

```python
splitter = RecursiveCharacterTextSplitter(
    separators=["\n"],
    chunk_size=2000,
    chunk_overlap=0,
)
doc_chunks = splitter.split_text(doc_content)
```

and, finally, ingest them into the vector store after attaching relevant metadata

```python
self._vectordb.add_documents(
    [
        Document(
            page_content=chunk,
            metadata={"document": doc_path, "paragraph": i + 1},
        )
        for i, chunk in enumerate(doc_chunks)
    ]
)
```

Note that each chunk is associated with the corresponding embeddings, as generated by the embedding model - this is taken care of automatically by LangChain's vector store implementation.

## Implementation: Inference

Before inference, the top 10 most relevant chunks to the user query are extracted from the vector store

```python
relevant_chunks = self._vectordb.similarity_search(question, k=10)
```

and used to create an augmented user message

```python
augmented_question = f"""Answer the following question using ONLY the information in the numbered paragraphs below.
You MUST cite which paragraph number(s) you used in your answer (e.g., "According to paragraph 3..."). If the answer is not in any paragraph, say 'I cannot answer based on the provided context.'

{"\n\n".join(f"{doc.metadata['paragraph']}. {doc.page_content}" for doc in relevant_chunks)}

Question: {question}"""
```

Notice the wording, specifically crafted to prevent the model from accessing its training data to obtain the answer, given that the text is from the public domain.

The augmented query is included in the conversation passed to the LLM

```python
response = self._llm.invoke(
    self._chat_history.messages + [augmented_question],
    config=self.get_config(ctx),
)
```

Note that the chat history records the original query, without any RAG context. This was moved to the end of `get_answer` to avoid exposing the query twice.

```python
self._chat_history.add_message(user_message(question))
self._chat_history.add_message(assistant_message(answer))
```

Thus, the chatbot's state is prepared for a future call.

## Verification

Ask questions about the document and observe which chunks are being retrieved and how the model interprets the content.
Try different embedding models and LLMs, local and remote, and assess how the quality of the response changes.

üè† [Overview](/README.md) | ‚óÄÔ∏è [Back to exercise](/src/chatbot/lessons/exercises/e07_rag/README.md) | ‚ñ∂Ô∏è [Next exercise](/src/chatbot/lessons/exercises/e08_custom_agent/README.md)
---|---|---
